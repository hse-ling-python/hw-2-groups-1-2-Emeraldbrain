{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pep8 in d:\\anaconda\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: pymystem3 in d:\\anaconda\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from pymystem3) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->pymystem3) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->pymystem3) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from requests->pymystem3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->pymystem3) (2019.6.16)\n",
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pep8\n",
    "! pip install pymystem3\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устанавливаем пакеты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from pprint import pprint\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем все, что нам может понадобиться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 и 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('infinity.txt', encoding='utf-8') as f:\n",
    "    book = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Открываем и читаем книгу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 49s\n"
     ]
    }
   ],
   "source": [
    "m = Mystem()\n",
    "%time analise_text = m.analyze(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замеряем время анализа книги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemmas.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(analise_text, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем новый файл, куда записыываем реузльтаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Alexandra\n",
      "[nltk_data]     Morozova\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без 'punkt' токенизация отказывается работать, поэтому устанавливаем её"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokens = word_tokenize(book)\n",
    "words = [w.lower() for w in tokens if w.isalpha()]\n",
    "lems = []\n",
    "\n",
    "for i in words:\n",
    "    analise = morph.parse(i)\n",
    "    lems.append(analise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем, удалив пункуацию. Замеряем время токинизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lem_form = []\n",
    "for w in lems:\n",
    "    first = w[0]\n",
    "    lem_form = [first.normal_form, first.tag.POS]\n",
    "    list_lem_form.append(lem_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемма и часть речи \"закидывается\" в отдельный массив. Без массива отказ записать результаты в файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lem_pymorph.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(list_lem_form, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем результаты в файл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN 0.24647\n",
      "VERB 0.17032\n",
      "None 0.00427\n",
      "GRND 0.01244\n",
      "PREP 0.0886\n",
      "CONJ 0.09324\n",
      "ADJF 0.08172\n",
      "NPRO 0.11813\n",
      "PRCL 0.04996\n",
      "ADVB 0.06185\n",
      "PRTF 0.00854\n",
      "INFN 0.0364\n",
      "NUMR 0.00186\n",
      "ADJS 0.01393\n",
      "PRED 0.00464\n",
      "COMP 0.00464\n",
      "INTJ 0.00093\n",
      "PRTS 0.00204\n"
     ]
    }
   ],
   "source": [
    "form_count = collections.Counter()\n",
    "summary = 0\n",
    "for i  in list_lem_form:\n",
    "    summary = summary + 1\n",
    "    form = i[1]\n",
    "    form_count[form] += 1\n",
    "\n",
    "for form in form_count:\n",
    "    print(form, round(form_count[form]/summary, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала считаем количество слов всех частей речи в тексте. Потом считаем и выводим долю каждой части речи относительно всех слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Глаголы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем в книге глаголы. Занесем их в массив. Выведем самые популярные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "быть 68\n",
      "мочь 28\n",
      "сказать 24\n",
      "стать 18\n",
      "думать 15\n",
      "посмотреть 13\n",
      "хотеть 13\n",
      "смотреть 11\n",
      "видеть 11\n",
      "знать 10\n",
      "спросить 9\n",
      "ответить 9\n",
      "смочь 9\n",
      "подойти 8\n",
      "сделать 8\n",
      "считать 8\n",
      "начать 6\n",
      "подняться 6\n",
      "упасть 6\n",
      "произойти 6\n"
     ]
    }
   ],
   "source": [
    "verb_count = collections.Counter()\n",
    "for i in list_lem_form:\n",
    "    if i[1] == 'VERB':\n",
    "        verb_count[i[0]] += 1\n",
    "        \n",
    "top_verb = verb_count.most_common(20)      \n",
    "for i in top_verb:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наречия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем в книге наречия. Занесем в массив. Выведем самые популярные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "уже 12\n",
      "сейчас 10\n",
      "совсем 9\n",
      "хорошо 9\n",
      "очень 9\n",
      "здесь 8\n",
      "медленно 8\n",
      "почему 8\n",
      "несколько 8\n",
      "потом 7\n",
      "тогда 6\n",
      "теперь 6\n",
      "всего 6\n",
      "вообще 6\n",
      "тут 5\n",
      "спокойно 5\n",
      "сюда 4\n",
      "где 4\n",
      "вдруг 4\n",
      "быстро 4\n"
     ]
    }
   ],
   "source": [
    "adv_count = collections.Counter()\n",
    "for i in list_lem_form:\n",
    "    if i[1] == 'ADVB':\n",
    "        adv_count[i[0]] += 1\n",
    "        \n",
    "top_adv = adv_count.most_common(20)      \n",
    "for i in top_adv:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как хорошо, что мы сделали третье задание. Возьмем же леммы оттуда!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "massiv = []\n",
    "for i in list_lem_form:\n",
    "    massiv.append(i[0])\n",
    "bi = nltk.bigrams(massiv)\n",
    "tri = nltk.trigrams(massiv)\n",
    "\n",
    "bi_count = collections.Counter(bi).most_common(25)\n",
    "tri_count = collections.Counter(tri).most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем частотность би- и триграмм из задания 3. Выводим ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самые частотные биграмы:\n",
      "('на', 'земля')\n",
      "('что', 'мы')\n",
      "('мы', 'не')\n",
      "('я', 'не')\n",
      "('мы', 'быть')\n",
      "('смотреть', 'на')\n",
      "('думать', 'что')\n",
      "('с', 'вы')\n",
      "('и', 'вы')\n",
      "('что', 'это')\n",
      "('мы', 'должный')\n",
      "('мочь', 'быть')\n",
      "('посмотреть', 'на')\n",
      "('в', 'рука')\n",
      "('вернуться', 'на')\n",
      "('не', 'знать')\n",
      "('не', 'хотеть')\n",
      "('не', 'быть')\n",
      "('ничто', 'не')\n",
      "('сказать', 'эллер')\n",
      "('подойти', 'к')\n",
      "('что', 'вы')\n",
      "('не', 'мочь')\n",
      "('блейк', 'мы')\n",
      "('что', 'я')\n"
     ]
    }
   ],
   "source": [
    "print('Самые частотные биграмы:')\n",
    "for i in bi_count:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самые частотные триграмы:\n",
      "('вернуться', 'на', 'земля')\n",
      "('на', 'земля', 'мы')\n",
      "('что', 'мы', 'не')\n",
      "('к', 'пульт', 'управление')\n",
      "('что', 'с', 'мы')\n",
      "('я', 'считать', 'что')\n",
      "('что', 'это', 'значит')\n",
      "('я', 'думать', 'что')\n",
      "('мы', 'должный', 'вернуться')\n",
      "('на', 'земля', 'и')\n",
      "('возвратиться', 'на', 'земля')\n",
      "('на', 'миллион', 'год')\n",
      "('держать', 'в', 'рука')\n",
      "('для', 'жизнь', 'но')\n",
      "('вы', 'ведь', 'капитан')\n",
      "('в', 'глубина', 'космос')\n",
      "('и', 'мочь', 'быть')\n",
      "('я', 'в', 'это')\n",
      "('возвращение', 'на', 'земля')\n",
      "('появиться', 'лицо', 'сильвий')\n",
      "('мы', 'выйти', 'на')\n",
      "('он', 'посмотреть', 'на')\n",
      "('с', 'мы', 'произойти')\n",
      "('весь', 'в', 'порядок')\n",
      "('указать', 'рука', 'на')\n"
     ]
    }
   ],
   "source": [
    "print('Самые частотные триграмы:')\n",
    "for i in tri_count:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводятся частотные для русского языка коснтрукции с предлогами, конструкции с именами героев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
